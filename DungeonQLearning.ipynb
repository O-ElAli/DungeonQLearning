{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/O-ElAli/DungeonQLearning/blob/main/DungeonQLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be6b992-85e9-4b9f-9f24-1a0ef29afe71",
      "metadata": {
        "id": "1be6b992-85e9-4b9f-9f24-1a0ef29afe71"
      },
      "source": [
        "# Introduction to Q-Learning\n",
        "Q-Learning is a model-free reinforcement learning method used to find the best *policy* in a given state in order to achieve the highest long-term reward.\n",
        "\n",
        "## Intuition:\n",
        "You are in an unknown maze, and your goal is to reach the exit. At first, you don’t know the best way to get there. So, you have to try different paths and learn which ones are better than others.\n",
        "\n",
        "Q-Learning helps you discover these “signposts” by allowing you to evaluate the quality or “value” of actions.\n",
        "\n",
        "## The Game We Want to Learn\n",
        "We will apply Q-Learning to a simple dungeon-crawling game that works as follows:\n",
        "\n",
        "### The Dungeon Layout:\n",
        "\n",
        "* **Size:** A 5x5 grid  \n",
        "* **Start position:** The player (🧙) starts in the southwest corner of the grid  \n",
        "* **Goal:** The target (🏁) is in the northeast corner  \n",
        "* **Fire:** Dangerous areas (🔥) in the dungeon that give negative points  \n",
        "* **Dragon:** A dragon (🐉) that kills the player if nearby  \n",
        "* **Treasure:** A treasure (💰) that grants a high reward when found  \n",
        "\n",
        "### Game Rules:\n",
        "\n",
        "* The player can move **north**, **south**, **west**, or **east**  \n",
        "* Each step gives a reward of **“-0.1” points** (a step costs something, so we want a short path to the goal)  \n",
        "* Contact with **fire** results in **“-10” points**  \n",
        "* Being **within 1 tile** of the **dragon** results in **“-∞” points** (the player dies)  \n",
        "* Stepping on the **treasure** grants **“+50” points**  \n",
        "* Reaching the **goal** gives **“+100” points** and ends the game\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11857e25-99de-44b1-95ad-b3564ac3058f",
      "metadata": {
        "id": "11857e25-99de-44b1-95ad-b3564ac3058f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74cca8a5-103a-4b9d-bf5d-b100e003f593",
      "metadata": {
        "id": "74cca8a5-103a-4b9d-bf5d-b100e003f593"
      },
      "source": [
        "# Definition of Constants and Creation of the Dungeon Grid\n",
        "\n",
        "In this section of the code, we define the various elements of our dungeon and create the grid that represents the dungeon. We also set the rewards and penalties for different events.\n",
        "\n",
        "We use a 5x5 NumPy array to define the positions of the different elements in the dungeon.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a0f937-a6b9-43bc-ab0d-2a88d42f7de1",
      "metadata": {
        "id": "74a0f937-a6b9-43bc-ab0d-2a88d42f7de1"
      },
      "outputs": [],
      "source": [
        "\n",
        "EMPTY = 0\n",
        "GOAL = 1\n",
        "FIRE = 2\n",
        "DRAGON = 3\n",
        "TREASURE = 4\n",
        "PLAYER = 5\n",
        "\n",
        "\n",
        "#dungeon.shape=(5,5)\n",
        "#(rows,columns) = dungeon.shape\n",
        "#dungeon.shape[0] -> # of rows\n",
        "#dungeon.shape[1] -> # of cols\n",
        "dungeon = np.array([\n",
        "    [EMPTY,  EMPTY, EMPTY, EMPTY, GOAL],\n",
        "    [DRAGON,  FIRE,  FIRE, EMPTY, EMPTY],\n",
        "    [EMPTY,  EMPTY, TREASURE, FIRE, EMPTY],\n",
        "    [EMPTY,  EMPTY, EMPTY,  EMPTY, EMPTY],\n",
        "    [PLAYER, FIRE, EMPTY, EMPTY, EMPTY]\n",
        "])\n",
        "\n",
        "GRID_SIZE = 5\n",
        "ACTIONS = ['N', 'S', 'W', 'E']\n",
        "ACTION_COUNT = len(ACTIONS)\n",
        "\n",
        "step_cost = -1\n",
        "fire_cost = -50\n",
        "dragon_cost = -np.inf\n",
        "treasure_reward = 30\n",
        "goal_reward = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ab2ea2-f695-4334-b30b-bd54e0e9f5d4",
      "metadata": {
        "id": "c4ab2ea2-f695-4334-b30b-bd54e0e9f5d4"
      },
      "outputs": [],
      "source": [
        "# Size is based on the dungeon size, and a flag treasure collected/not collected\n",
        "Q_table = np.zeros((GRID_SIZE * GRID_SIZE * 2, ACTION_COUNT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0eac31c-d85a-4046-a791-9b36eb0319d6",
      "metadata": {
        "id": "e0eac31c-d85a-4046-a791-9b36eb0319d6",
        "outputId": "063f90dd-e308-4823-907b-817f7e91d0ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌───┬───┬───┬───┬───┐\n",
            "│   │   │   │   │🏁 │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│🐉 │🔥 │🔥 │   │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │   │💰 │🔥 │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │   │   │   │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│🧙 │🔥 │   │   │   │\n",
            "└───┴───┴───┴───┴───┘\n"
          ]
        }
      ],
      "source": [
        "symbol_map = {\n",
        "    EMPTY: '   ',  # Empty space\n",
        "    GOAL: '🏁 ',\n",
        "    FIRE: '🔥 ',\n",
        "    DRAGON: '🐉 ',\n",
        "    TREASURE: '💰 ',\n",
        "    PLAYER: '🧙 '\n",
        "}\n",
        "\n",
        "#dungeon.shape=(5,5)\n",
        "#(rows,columns) = dungeon.shape\n",
        "#dungeon.shape[0] -> # of rows\n",
        "#dungeon.shape[1] -> # of cols\n",
        "\n",
        "def print_dungeon(dungeon):\n",
        "\n",
        "    top_border = \"┌───\" + \"┬───\" * (dungeon.shape[1] - 1) + \"┐\" # --> 5-1 = 4\n",
        "    middle_border = \"├───\" + \"┼───\" * (dungeon.shape[1] - 1) + \"┤\"\n",
        "    bottom_border = \"└───\" + \"┴───\" * (dungeon.shape[1] - 1) + \"┘\"\n",
        "\n",
        "#.join is for tuples\n",
        "#myTuple = (\"John\", \"Peter\", \"Vicky\")\n",
        "#x = \"#\".join(myTuple)\n",
        "# RESULT x--> John#Peter#Vicky\n",
        "\n",
        "    print(top_border)\n",
        "    for i, row in enumerate(dungeon):\n",
        "        #print(\"i=\",i,\"row=\",row)\n",
        "        #i=row number (0,1,2,3,4), row=organization of cells inside the row (empty, empty, empty, empty, goal) also referred in numbers (0,0,0,0,1)\n",
        "        #the loop is just the different symbols being printed\n",
        "        row_str = \"│\" + \"│\".join(symbol_map[cell] for cell in row) + \"│\"\n",
        "        print(row_str)\n",
        "        if i < dungeon.shape[0] - 1: #if i (row we're at) is less than max number of rows:\n",
        "            print(middle_border)\n",
        "    print(bottom_border)\n",
        "\n",
        "# Print the dungeon\n",
        "print_dungeon(dungeon)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1370ac2-d74f-46f7-b9a9-a82b00a2dfd4",
      "metadata": {
        "id": "f1370ac2-d74f-46f7-b9a9-a82b00a2dfd4"
      },
      "source": [
        "## How Does a Q-Learning Episode Work?\n",
        "In one episode, the agent takes several steps to learn how to behave in an environment.\n",
        "\n",
        "It uses the following parameters:\n",
        "\n",
        "### Parameter Explanations:\n",
        "* **$\\alpha$ (Learning Rate)**: Determines how much new information overrides the old Q-value. A high value means new information is given more weight.\n",
        "* **$\\gamma$ (Discount Factor)**: Indicates how important future rewards are. A value close to 1 means future rewards are highly considered.\n",
        "* **$\\epsilon$ (Exploration Rate)**: Determines how often the agent chooses random actions (*exploration*) instead of relying on the best known action (*exploitation*). At the beginning, epsilon is high so the agent can better explore the environment.\n",
        "\n",
        "### Procedure:\n",
        "- Choose an action from the action space:\n",
        "    - With probability `epsilon`: Choose a random valid action (*exploration*)\n",
        "    - Otherwise: Choose the valid action with the highest Q-value in the current state (*exploitation*)\n",
        "\n",
        "- Execute the chosen action and observe the new state and the reward received\n",
        "\n",
        "- Update the Q-value for the current state and action using the *Q-Learning formula*:\n",
        "    - $ Q(s, a) \\leftarrow (1 - \\alpha) \\cdot Q(s, a) + \\alpha \\cdot \\left[ r + \\gamma \\cdot \\max_{a'} Q(s', a') \\right] $\n",
        "\n",
        "        * $Q(s,a)$: Q-value for state $s$ and action $a$  \n",
        "        * $r$: Reward for taking action $a$ in state $s$  \n",
        "        * $s'$: New state reached from $s$ by taking action $a$\n",
        "\n",
        "- Set the new state $s'$ as the current state\n",
        "\n",
        "- If the goal state is reached or the agent dies (e.g., from the dragon):  \n",
        "    - End the episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb4386e8-f837-4857-b2e9-84a61cfb3066",
      "metadata": {
        "id": "fb4386e8-f837-4857-b2e9-84a61cfb3066"
      },
      "outputs": [],
      "source": [
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.5  # Exploration\n",
        "episodes = 50000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89bc1027-474d-4c2f-a49a-1c6b76578816",
      "metadata": {
        "id": "89bc1027-474d-4c2f-a49a-1c6b76578816"
      },
      "outputs": [],
      "source": [
        "#state is the position of the player, starts at (4,0) or southwest\n",
        "def get_state_index(state, tcollected):\n",
        "    x, y = state #new x and why values\n",
        "    #tcollected=treasure collected, adds 1 if true and 0 if false\n",
        "    return (x * GRID_SIZE + y) * 2 + int(tcollected)\n",
        "    #we return the current value of the grid/state in one number\n",
        "    # Example: starting position (4, 0), GRID_SIZE = 5, no treasure collected\n",
        "    # Calculation: (4 * 5 + 0) * 2 + 0 = 20 * 2 + 0 = 40\n",
        "\n",
        "#up and left = -1\n",
        "#down and right = +1\n",
        "def get_new_state(state, action):\n",
        "    x, y = state\n",
        "    #starting state = (4,0)\n",
        "    if action == 0:  # North\n",
        "        new_state = (max(x - 1, 0), y) #new_state=(3,0) max until x-1=-1\n",
        "    elif action == 1:  # South\n",
        "        new_state = (min(x + 1, GRID_SIZE - 1), y) #new_state=(4,0)\n",
        "    elif action == 2:  # West\n",
        "        new_state = (x, max(y - 1, 0))#new_state=(4,0)\n",
        "    elif action == 3:  # East\n",
        "        new_state = (x, min(y + 1, GRID_SIZE - 1))#new_state=(1,1)\n",
        "\n",
        "    return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebdcf19-5161-4952-bd75-03ba425606a2",
      "metadata": {
        "id": "0ebdcf19-5161-4952-bd75-03ba425606a2"
      },
      "outputs": [],
      "source": [
        "def get_valid_actions(state):\n",
        "    x, y = state\n",
        "    valid_actions = []\n",
        "    #variable containing a list of all actions the player CAN perform\n",
        "    #starting position (4,0) --> valid_actions = [0,3]\n",
        "    #middle of grid (2,2) --> valid_actions = [0,1,2,3]\n",
        "    if x > 0: valid_actions.append(0)  # North\n",
        "    if x < GRID_SIZE - 1: valid_actions.append(1)  # South\n",
        "    if y > 0: valid_actions.append(2)  # West\n",
        "    if y < GRID_SIZE - 1: valid_actions.append(3)  # East\n",
        "    return valid_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a495a1b-7be3-4983-b9fb-e4b7b99c93ea",
      "metadata": {
        "id": "7a495a1b-7be3-4983-b9fb-e4b7b99c93ea"
      },
      "outputs": [],
      "source": [
        "def get_cost(new_state):\n",
        "              #     x             y\n",
        "    if dungeon[new_state[0], new_state[1]] == GOAL:\n",
        "        return goal_reward\n",
        "\n",
        "#python syntax, but the loop runs first then checks if the player has any dragons around him in a 1 block radius\n",
        "    if any(dungeon[new_state[0] + dx, new_state[1] + dy] == DRAGON\n",
        "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)] #each tuple gets unpacked into dx and dy: (-1,0) -> dx=-1, dy=0\n",
        "            if 0 <= new_state[0] + dx < GRID_SIZE\n",
        "            and 0 <= new_state[1] + dy < GRID_SIZE):\n",
        "        return dragon_cost\n",
        "\n",
        "    #if player is on a block that has a chest that wasnt collected add a reward\n",
        "    if dungeon[new_state[0], new_state[1]] == TREASURE and not tcollected:\n",
        "        return treasure_reward\n",
        "\n",
        "    if dungeon[new_state[0], new_state[1]] == FIRE:\n",
        "        return fire_cost\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c896c0bd-ff2f-4f48-b2d5-62b54625aa62",
      "metadata": {
        "id": "c896c0bd-ff2f-4f48-b2d5-62b54625aa62"
      },
      "outputs": [],
      "source": [
        "def choose_action(state, tcollected, valid_actions, Q_table, epsilon):\n",
        "    state_index = get_state_index(state, tcollected)\n",
        "    #random float between 0 and 1\n",
        "    #if more than epsilon, take a random decision for the available actions\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(valid_actions)  # Explore: select a random valid action\n",
        "    else:\n",
        "        # Exploit: select the best action based on the Q-table\n",
        "        #sending the state_index to show where the player is\n",
        "        #qtable shows what how \"valuable\" each subsequent option of the player is\n",
        "        #state(4,0) -> state_index=40\n",
        "        #qtable[40] = [0.5,-1.0,0.0,0.2] representing [North, South, West, East]\n",
        "        #np.argmax returns the index (action) with the highest value → 0 in this case (North)\n",
        "        return np.argmax(Q_table[state_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097655b4-a28a-4719-a39e-4621c63555eb",
      "metadata": {
        "id": "097655b4-a28a-4719-a39e-4621c63555eb"
      },
      "outputs": [],
      "source": [
        "def update_Q_table(state, new_state, tcollected):\n",
        "        state_index = get_state_index(state, tcollected) #reminder: tcollected is a boolean that adds 1 or 0 to the state index\n",
        "        #example: state(4,0), state_index=40\n",
        "        new_state_index = get_state_index(new_state, tcollected)\n",
        "        #example (no treasure): new_state(4,1), new_state_index= 42\n",
        "        reward = step_cost\n",
        "        #every step starts with a negative reward\n",
        "\n",
        "        #               (0      ,       4)\n",
        "        if dungeon[new_state[0], new_state[1]] == GOAL:\n",
        "            Q_table[state_index, action] = goal_reward\n",
        "            #add to the table current location/state_index plus the decision taken to the qtable so future runs know where to go\n",
        "            #example: state(0,3), action east (0,1), add to qtable 100 points as reward\n",
        "            return False, tcollected #return false to end the game\n",
        "\n",
        "\n",
        "        #checks if dragon is in a one block radius. If yes, game over\n",
        "        if any(dungeon[new_state[0] + dx, new_state[1] + dy] == DRAGON\n",
        "               for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "               if 0 <= new_state[0] + dx < GRID_SIZE\n",
        "               and 0 <= new_state[1] + dy < GRID_SIZE):\n",
        "            Q_table[state_index, action] = dragon_cost\n",
        "            return False, tcollected\n",
        "        #if player current state is a treasure that hasn't been collected, add reward and mark as collected\n",
        "        if dungeon[new_state[0], new_state[1]] == TREASURE and not tcollected:\n",
        "            reward += treasure_reward\n",
        "            tcollected = True  # Mark treasure as collected\n",
        "            new_state_index = get_state_index(new_state, tcollected)\n",
        "        #if player current state is a fire, add fire cost\n",
        "        elif dungeon[new_state[0], new_state[1]] == FIRE:\n",
        "            reward += fire_cost\n",
        "        #updating the qtable\n",
        "        #saving old value first for the equation\n",
        "        old_value = Q_table[state_index, action] #qtable[40] = [0.5,-1.0,0.0,0.2] --> old value = 0.5\n",
        "        #saving the new_state_index qtable reward values\n",
        "        future_value = np.max(Q_table[new_state_index])#qtable[38] = [0.2,0.0,0.0,0.3] --> future value = 0.3\n",
        "\n",
        "\n",
        "        Q_table[state_index, action] = (1 - alpha) * old_value\n",
        "\n",
        "        Q_table[state_index, action] += alpha * (reward + gamma * future_value)\n",
        "\n",
        "        return True, tcollected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "383deaad-17cd-494b-a85e-bc0e967bf913",
      "metadata": {
        "id": "383deaad-17cd-494b-a85e-bc0e967bf913",
        "outputId": "d21173c0-775c-4698-d3fd-3d1c8bd23ad4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:03<00:00, 13030.57it/s]\n"
          ]
        }
      ],
      "source": [
        "for episode in tqdm(range(episodes)):\n",
        "    state = (4, 0)  # Startin where the player is initally\n",
        "    tcollected = False\n",
        "\n",
        "    run = True\n",
        "    while run:\n",
        "        valid_actions = get_valid_actions(state)\n",
        "        action = choose_action(state, tcollected,\n",
        "                               valid_actions, Q_table, epsilon)\n",
        "\n",
        "        new_state = get_new_state(state, action)\n",
        "\n",
        "        run, tcollected = update_Q_table(state,\n",
        "                                 new_state, tcollected)\n",
        "\n",
        "        state = new_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f345191-0d2e-4ef7-a449-6de89a025c20",
      "metadata": {
        "id": "7f345191-0d2e-4ef7-a449-6de89a025c20"
      },
      "source": [
        "### Computed Policy\n",
        "After Q-Learning has been completed and the Q-table is filled with the optimal Q-values, we can extract the best action for each state and use it to create the policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4359dc61-5700-4088-b703-3f2e08a101c9",
      "metadata": {
        "id": "4359dc61-5700-4088-b703-3f2e08a101c9",
        "outputId": "011abe16-59af-42b8-ef39-02162db04d3d"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m policy:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(row))\n\u001b[0;32m---> 30\u001b[0m werte_not_collected, policy_not_collected \u001b[38;5;241m=\u001b[39m \u001b[43mextract_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcollected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m werte_collected, policy_collected \u001b[38;5;241m=\u001b[39m extract_policy(Q_table, tcollected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m print_policy(werte_not_collected, policy_not_collected, treasure_status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mextract_policy\u001b[0;34m(Q_table, tcollected)\u001b[0m\n\u001b[1;32m     14\u001b[0m         best_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q_table[state_index])\n\u001b[1;32m     15\u001b[0m         policy[x, y] \u001b[38;5;241m=\u001b[39m action_map[best_action]\n\u001b[0;32m---> 16\u001b[0m         werte[x, y] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(Q_table[state_index])\u001b[38;5;241m+\u001b[39m\u001b[43mget_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m werte, policy\n",
            "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mget_cost\u001b[0;34m(new_state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cost\u001b[39m(new_state):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dungeon[\u001b[43mnew_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, new_state[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m==\u001b[39m GOAL:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m goal_reward\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(dungeon[new_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m dx, new_state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m dy] \u001b[38;5;241m==\u001b[39m DRAGON \n\u001b[1;32m      6\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m dx, dy \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)] \n\u001b[1;32m      7\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m new_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m dx \u001b[38;5;241m<\u001b[39m GRID_SIZE \n\u001b[1;32m      8\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m new_state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m dy \u001b[38;5;241m<\u001b[39m GRID_SIZE):\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "def extract_policy(Q_table, tcollected):\n",
        "    action_map = {\n",
        "        0: 'N',  # North\n",
        "        1: 'S',  # South\n",
        "        2: 'W',  # West\n",
        "        3: 'E'   # East\n",
        "    }\n",
        "    werte = np.zeros((GRID_SIZE, GRID_SIZE), dtype=float)\n",
        "    policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=str)\n",
        "\n",
        "    for x in range(GRID_SIZE):\n",
        "        for y in range(GRID_SIZE):\n",
        "            state_index = get_state_index( (x, y), tcollected)\n",
        "            best_action = np.argmax(Q_table[state_index])\n",
        "            policy[x, y] = action_map[best_action]\n",
        "            werte[x, y] = np.max(Q_table[state_index])+get_cost(state_index)\n",
        "\n",
        "    return werte, policy\n",
        "\n",
        "def print_policy(werte, policy, treasure_status):\n",
        "    np.set_printoptions(formatter={'all':lambda x: \"{:6.2f}\".format(x)})\n",
        "    print(f\"\\nQ-Werte (Treasure {'Collected' if treasure_status else 'Not Collected'}):\")\n",
        "    for row in werte:\n",
        "        print(row.round(2))\n",
        "\n",
        "    print(f\"\\nOptimal Policy (Treasure {'Collected' if treasure_status else 'Not Collected'}):\")\n",
        "    for row in policy:\n",
        "        print(\" \".join(row))\n",
        "\n",
        "werte_not_collected, policy_not_collected = extract_policy(Q_table, tcollected=False)\n",
        "werte_collected, policy_collected = extract_policy(Q_table, tcollected=True)\n",
        "\n",
        "print_policy(werte_not_collected, policy_not_collected, treasure_status=False)\n",
        "print_policy(werte_collected, policy_collected, treasure_status=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c12ddb2-0d3e-4fff-85a7-960260ffe93c",
      "metadata": {
        "id": "6c12ddb2-0d3e-4fff-85a7-960260ffe93c"
      },
      "source": [
        "### Executing the Policy\n",
        "The following code visualizes the policy and guides our player to the goal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef102a9-26e8-43a0-b2ee-173413d61099",
      "metadata": {
        "id": "7ef102a9-26e8-43a0-b2ee-173413d61099"
      },
      "outputs": [],
      "source": [
        "def animate_step(dungeon, player_pos, tcollected):\n",
        "    display_dungeon = np.copy(dungeon)\n",
        "\n",
        "    px, py = player_pos\n",
        "\n",
        "    if tcollected and display_dungeon[px, py] == TREASURE:\n",
        "        display_dungeon[px, py] = EMPTY\n",
        "\n",
        "    display_dungeon[px, py] = PLAYER\n",
        "\n",
        "    top_border = \"┌───\" + \"┬───\" * (dungeon.shape[1] - 1) + \"┐\"\n",
        "    middle_border = \"├───\" + \"┼───\" * (dungeon.shape[1] - 1) + \"┤\"\n",
        "    bottom_border = \"└───\" + \"┴───\" * (dungeon.shape[1] - 1) + \"┘\"\n",
        "\n",
        "    print_grid = [top_border]\n",
        "\n",
        "    for x in range(GRID_SIZE):\n",
        "        row = '│'\n",
        "        for y in range(GRID_SIZE):\n",
        "            if tcollected and display_dungeon[x, y] == TREASURE:\n",
        "                row += symbol_map[EMPTY] + '│'  # Treasure is hidden once collected\n",
        "            else:\n",
        "                row += symbol_map[display_dungeon[x, y]] + '│'\n",
        "        print_grid.append(row)\n",
        "        if x < GRID_SIZE - 1:\n",
        "            print_grid.append(middle_border)\n",
        "\n",
        "    print_grid.append(bottom_border)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    for line in print_grid:\n",
        "        print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2affcc8-fb97-4f77-9efb-3b6965375887",
      "metadata": {
        "id": "a2affcc8-fb97-4f77-9efb-3b6965375887"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def animate_policy(Q_table, dungeon, start_pos=(4, 0), sleep_time=2):\n",
        "    werte_not_collected, policy_not_collected = extract_policy(Q_table, tcollected=False)\n",
        "    werte_collected, policy_collected = extract_policy(Q_table, tcollected=True)\n",
        "\n",
        "    player_pos = start_pos\n",
        "    tcollected = False\n",
        "    accumulated_reward = 0\n",
        "\n",
        "    px, py = player_pos\n",
        "    dungeon[px, py] = EMPTY  # Start by clearing the player’s initial position\n",
        "\n",
        "    while True:\n",
        "        animate_step(dungeon, player_pos, tcollected)\n",
        "\n",
        "        state_index = get_state_index(player_pos, tcollected)\n",
        "        best_action = np.argmax(Q_table[state_index])\n",
        "\n",
        "        new_pos = get_new_state(player_pos, best_action)\n",
        "        x, y = new_pos\n",
        "\n",
        "        if dungeon[x, y] == TREASURE:\n",
        "            tcollected = True\n",
        "            dungeon[x, y] = EMPTY  # Mark treasure as collected\n",
        "\n",
        "        if dungeon[x, y] == GOAL:\n",
        "            clear_output(wait=True)\n",
        "            animate_step(dungeon, new_pos, tcollected)\n",
        "            print(\"Reached goal!!\")\n",
        "            break\n",
        "\n",
        "        if dungeon[x, y] == DRAGON:\n",
        "            clear_output(wait=True)\n",
        "            animate_step(dungeon, new_pos, tcollected)\n",
        "            print(\"Died to the Dragon!!\")\n",
        "            break\n",
        "\n",
        "        player_pos = new_pos\n",
        "\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "copied_dungeon = np.copy(dungeon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0506353-ed94-4d3e-bc02-456f0bd94687",
      "metadata": {
        "id": "e0506353-ed94-4d3e-bc02-456f0bd94687",
        "outputId": "03d78894-2bd3-4388-c6b9-7a984e771fb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "┌───┬───┬───┬───┬───┐\n",
            "│   │   │   │   │🧙 │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│🐉 │🔥 │🔥 │   │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │   │   │🔥 │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │   │   │   │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │🔥 │   │   │   │\n",
            "└───┴───┴───┴───┴───┘\n",
            "Reached goal!!\n"
          ]
        }
      ],
      "source": [
        "animate_policy(Q_table, copied_dungeon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d5b8d1-7bd5-4b6f-96d7-7016fe7f43b8",
      "metadata": {
        "id": "c2d5b8d1-7bd5-4b6f-96d7-7016fe7f43b8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}