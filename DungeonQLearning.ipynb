{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/O-ElAli/DungeonQLearning/blob/main/DungeonQLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be6b992-85e9-4b9f-9f24-1a0ef29afe71",
      "metadata": {
        "id": "1be6b992-85e9-4b9f-9f24-1a0ef29afe71"
      },
      "source": [
        "**bold text**\n",
        "# Introduction to Q-Learning\n",
        "Q-Learning is a model-free reinforcement learning method used to find the best *policy* in a given state in order to achieve the highest long-term reward.\n",
        "\n",
        "## Intuition:\n",
        "You are in an unknown maze, and your goal is to reach the exit. At first, you don’t know the best way to get there. So, you have to try different paths and learn which ones are better than others.\n",
        "\n",
        "Q-Learning helps you discover these “signposts” by allowing you to evaluate the quality or “value” of actions.\n",
        "\n",
        "## The Game We Want to Learn\n",
        "We will apply Q-Learning to a simple dungeon-crawling game that works as follows:\n",
        "\n",
        "### The Dungeon Layout:\n",
        "\n",
        "* **Size:** A 5x5 grid  \n",
        "* **Start position:** The player (🧙) starts in the southwest corner of the grid  \n",
        "* **Goal:** The target (🏁) is in the northeast corner  \n",
        "* **Fire:** Dangerous areas (🔥) in the dungeon that give negative points  \n",
        "* **Dragon:** A dragon (🐉) that kills the player if nearby  \n",
        "* **Treasure:** A treasure (💰) that grants a high reward when found  \n",
        "\n",
        "### Game Rules:\n",
        "\n",
        "* The player can move **north**, **south**, **west**, or **east**  \n",
        "* Each step gives a reward of **“-0.1” points** (a step costs something, so we want a short path to the goal)  \n",
        "* Contact with **fire** results in **“-10” points**  \n",
        "* Being **within 1 tile** of the **dragon** results in **“-∞” points** (the player dies)  \n",
        "* Stepping on the **treasure** grants **“+50” points**  \n",
        "* Reaching the **goal** gives **“+100” points** and ends the game\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "11857e25-99de-44b1-95ad-b3564ac3058f",
      "metadata": {
        "id": "11857e25-99de-44b1-95ad-b3564ac3058f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74cca8a5-103a-4b9d-bf5d-b100e003f593",
      "metadata": {
        "id": "74cca8a5-103a-4b9d-bf5d-b100e003f593"
      },
      "source": [
        "# Definition of Constants and Creation of the Dungeon Grid\n",
        "\n",
        "In this section of the code, we define the various elements of our dungeon and create the grid that represents the dungeon. We also set the rewards and penalties for different events.\n",
        "\n",
        "We use a 5x5 NumPy array to define the positions of the different elements in the dungeon.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "74a0f937-a6b9-43bc-ab0d-2a88d42f7de1",
      "metadata": {
        "id": "74a0f937-a6b9-43bc-ab0d-2a88d42f7de1"
      },
      "outputs": [],
      "source": [
        "\n",
        "EMPTY = 0\n",
        "GOAL = 1\n",
        "FIRE = 2\n",
        "DRAGON = 3\n",
        "TREASURE = 4\n",
        "PLAYER = 5\n",
        "\n",
        "\n",
        "#dungeon.shape=(5,5)\n",
        "#(rows,columns) = dungeon.shape\n",
        "#dungeon.shape[0] -> # of rows\n",
        "#dungeon.shape[1] -> # of cols\n",
        "dungeon = np.array([\n",
        "    [EMPTY,  EMPTY, EMPTY, EMPTY, GOAL],\n",
        "    [DRAGON,  FIRE,  FIRE, EMPTY, EMPTY],\n",
        "    [EMPTY,  EMPTY, TREASURE, FIRE, EMPTY],\n",
        "    [EMPTY,  EMPTY, EMPTY,  EMPTY, EMPTY],\n",
        "    [PLAYER, FIRE, EMPTY, EMPTY, EMPTY]\n",
        "])\n",
        "\n",
        "GRID_SIZE = 5\n",
        "ACTIONS = ['N', 'S', 'W', 'E']\n",
        "ACTION_COUNT = len(ACTIONS)\n",
        "\n",
        "step_cost = -1\n",
        "fire_cost = -50\n",
        "dragon_cost = -np.inf\n",
        "treasure_reward = 30\n",
        "goal_reward = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c4ab2ea2-f695-4334-b30b-bd54e0e9f5d4",
      "metadata": {
        "id": "c4ab2ea2-f695-4334-b30b-bd54e0e9f5d4"
      },
      "outputs": [],
      "source": [
        "# Size is based on the dungeon size, and a flag treasure collected/not collected\n",
        "Q_table = np.zeros((GRID_SIZE * GRID_SIZE * 2, ACTION_COUNT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e0eac31c-d85a-4046-a791-9b36eb0319d6",
      "metadata": {
        "id": "e0eac31c-d85a-4046-a791-9b36eb0319d6",
        "outputId": "a1d571fd-2e16-4b26-e661-7d47d4f5899b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌───┬───┬───┬───┬───┐\n",
            "│   │   │   │   │🏁 │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│🐉 │🔥 │🔥 │   │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │   │💰 │🔥 │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │   │   │   │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│🧙 │🔥 │   │   │   │\n",
            "└───┴───┴───┴───┴───┘\n"
          ]
        }
      ],
      "source": [
        "symbol_map = {\n",
        "    EMPTY: '   ',  # Empty space\n",
        "    GOAL: '🏁 ',\n",
        "    FIRE: '🔥 ',\n",
        "    DRAGON: '🐉 ',\n",
        "    TREASURE: '💰 ',\n",
        "    PLAYER: '🧙 '\n",
        "}\n",
        "\n",
        "#dungeon.shape=(5,5)\n",
        "#(rows,columns) = dungeon.shape\n",
        "#dungeon.shape[0] -> # of rows\n",
        "#dungeon.shape[1] -> # of cols\n",
        "\n",
        "def print_dungeon(dungeon):\n",
        "\n",
        "    top_border = \"┌───\" + \"┬───\" * (dungeon.shape[1] - 1) + \"┐\" # --> 5-1 = 4\n",
        "    middle_border = \"├───\" + \"┼───\" * (dungeon.shape[1] - 1) + \"┤\"\n",
        "    bottom_border = \"└───\" + \"┴───\" * (dungeon.shape[1] - 1) + \"┘\"\n",
        "\n",
        "#.join is for tuples\n",
        "#myTuple = (\"John\", \"Peter\", \"Vicky\")\n",
        "#x = \"#\".join(myTuple)\n",
        "# RESULT x--> John#Peter#Vicky\n",
        "\n",
        "    print(top_border)\n",
        "    for i, row in enumerate(dungeon):\n",
        "        #print(\"i=\",i,\"row=\",row)\n",
        "        #i=row number (0,1,2,3,4), row=organization of cells inside the row (empty, empty, empty, empty, goal) also referred in numbers (0,0,0,0,1)\n",
        "        #the loop is just the different symbols being printed\n",
        "        row_str = \"│\" + \"│\".join(symbol_map[cell] for cell in row) + \"│\"\n",
        "        print(row_str)\n",
        "        if i < dungeon.shape[0] - 1: #if i (row we're at) is less than max number of rows:\n",
        "            print(middle_border)\n",
        "    print(bottom_border)\n",
        "\n",
        "# Print the dungeon\n",
        "print_dungeon(dungeon)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1370ac2-d74f-46f7-b9a9-a82b00a2dfd4",
      "metadata": {
        "id": "f1370ac2-d74f-46f7-b9a9-a82b00a2dfd4"
      },
      "source": [
        "## How Does a Q-Learning Episode Work?\n",
        "In one episode, the agent takes several steps to learn how to behave in an environment.\n",
        "\n",
        "It uses the following parameters:\n",
        "\n",
        "### Parameter Explanations:\n",
        "* **$\\alpha$ (Learning Rate)**: Determines how much new information overrides the old Q-value. A high value means new information is given more weight.\n",
        "* **$\\gamma$ (Discount Factor)**: Indicates how important future rewards are. A value close to 1 means future rewards are highly considered.\n",
        "* **$\\epsilon$ (Exploration Rate)**: Determines how often the agent chooses random actions (*exploration*) instead of relying on the best known action (*exploitation*). At the beginning, epsilon is high so the agent can better explore the environment.\n",
        "\n",
        "### Procedure:\n",
        "- Choose an action from the action space:\n",
        "    - With probability `epsilon`: Choose a random valid action (*exploration*)\n",
        "    - Otherwise: Choose the valid action with the highest Q-value in the current state (*exploitation*)\n",
        "\n",
        "- Execute the chosen action and observe the new state and the reward received\n",
        "\n",
        "- Update the Q-value for the current state and action using the *Q-Learning formula*:\n",
        "    - $ Q(s, a) \\leftarrow (1 - \\alpha) \\cdot Q(s, a) + \\alpha \\cdot \\left[ r + \\gamma \\cdot \\max_{a'} Q(s', a') \\right] $\n",
        "\n",
        "        * $Q(s,a)$: Q-value for state $s$ and action $a$  \n",
        "        * $r$: Reward for taking action $a$ in state $s$  \n",
        "        * $s'$: New state reached from $s$ by taking action $a$\n",
        "\n",
        "- Set the new state $s'$ as the current state\n",
        "\n",
        "- If the goal state is reached or the agent dies (e.g., from the dragon):  \n",
        "    - End the episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "fb4386e8-f837-4857-b2e9-84a61cfb3066",
      "metadata": {
        "id": "fb4386e8-f837-4857-b2e9-84a61cfb3066"
      },
      "outputs": [],
      "source": [
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.5  # Exploration\n",
        "episodes = 50000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "89bc1027-474d-4c2f-a49a-1c6b76578816",
      "metadata": {
        "id": "89bc1027-474d-4c2f-a49a-1c6b76578816"
      },
      "outputs": [],
      "source": [
        "#state is the position of the player, starts at (4,0) or southwest\n",
        "def get_state_index(state, tcollected):\n",
        "    x, y = state #new x and why values\n",
        "    #tcollected=treasure collected, adds 1 if true and 0 if false\n",
        "    return (x * GRID_SIZE + y) * 2 + int(tcollected)\n",
        "    #we return the current value of the grid/state in one number\n",
        "    # Example: starting position (4, 0), GRID_SIZE = 5, no treasure collected\n",
        "    # Calculation: (4 * 5 + 0) * 2 + 0 = 20 * 2 + 0 = 40\n",
        "\n",
        "#up and left = -1\n",
        "#down and right = +1\n",
        "def get_new_state(state, action):\n",
        "    x, y = state\n",
        "    #starting state = (4,0)\n",
        "    if action == 0:  # North\n",
        "        new_state = (max(x - 1, 0), y) #new_state=(3,0) max until x-1=-1\n",
        "    elif action == 1:  # South\n",
        "        new_state = (min(x + 1, GRID_SIZE - 1), y) #new_state=(4,0)\n",
        "    elif action == 2:  # West\n",
        "        new_state = (x, max(y - 1, 0))#new_state=(4,0)\n",
        "    elif action == 3:  # East\n",
        "        new_state = (x, min(y + 1, GRID_SIZE - 1))#new_state=(1,1)\n",
        "\n",
        "    return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0ebdcf19-5161-4952-bd75-03ba425606a2",
      "metadata": {
        "id": "0ebdcf19-5161-4952-bd75-03ba425606a2"
      },
      "outputs": [],
      "source": [
        "def get_valid_actions(state):\n",
        "    x, y = state\n",
        "    valid_actions = []\n",
        "    #variable containing a list of all actions the player CAN perform\n",
        "    #starting position (4,0) --> valid_actions = [0,3]\n",
        "    #middle of grid (2,2) --> valid_actions = [0,1,2,3]\n",
        "    if x > 0: valid_actions.append(0)  # North\n",
        "    if x < GRID_SIZE - 1: valid_actions.append(1)  # South\n",
        "    if y > 0: valid_actions.append(2)  # West\n",
        "    if y < GRID_SIZE - 1: valid_actions.append(3)  # East\n",
        "    return valid_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "7a495a1b-7be3-4983-b9fb-e4b7b99c93ea",
      "metadata": {
        "id": "7a495a1b-7be3-4983-b9fb-e4b7b99c93ea"
      },
      "outputs": [],
      "source": [
        "def get_cost(new_state, tcollected): #error in code, previous version tcollected was not passed but it was still used causing the code to fail later on\n",
        "    #another issue: new_state is expected to be a tuple here but it received an integer during call\n",
        "              #     x             y\n",
        "    if dungeon[new_state[0], new_state[1]] == GOAL:\n",
        "        return goal_reward\n",
        "\n",
        "    #python syntax, but the loop runs first then checks if the player has any dragons around him in a 1 block radius\n",
        "    if any(dungeon[new_state[0] + dx, new_state[1] + dy] == DRAGON\n",
        "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)] #each tuple gets unpacked into dx and dy: (-1,0) -> dx=-1, dy=0\n",
        "            if 0 <= new_state[0] + dx < GRID_SIZE\n",
        "            and 0 <= new_state[1] + dy < GRID_SIZE):\n",
        "        return dragon_cost\n",
        "\n",
        "    #if player is on a block that has a chest that wasn't collected add a reward\n",
        "    if dungeon[new_state[0], new_state[1]] == TREASURE and not tcollected:\n",
        "        return treasure_reward\n",
        "\n",
        "    if dungeon[new_state[0], new_state[1]] == FIRE:\n",
        "        return fire_cost\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c896c0bd-ff2f-4f48-b2d5-62b54625aa62",
      "metadata": {
        "id": "c896c0bd-ff2f-4f48-b2d5-62b54625aa62"
      },
      "outputs": [],
      "source": [
        "def choose_action(state, tcollected, valid_actions, Q_table, epsilon):\n",
        "    state_index = get_state_index(state, tcollected)\n",
        "    #random float between 0 and 1\n",
        "    #if more than epsilon, take a random decision for the available actions\n",
        "    if random.uniform(0, 1) < epsilon: #epsilon=0.5\n",
        "        return random.choice(valid_actions)  # Explore: select a random valid action\n",
        "    else:\n",
        "        # Exploit: select the best action based on the Q-table\n",
        "        #sending the state_index to show where the player is\n",
        "        #qtable shows what how \"valuable\" each subsequent option of the player is\n",
        "        #state(4,0) -> state_index=40\n",
        "        #qtable[40] = [0.5,-1.0,0.0,0.2] representing [North, South, West, East]\n",
        "        #np.argmax returns the index (action) with the highest value → 0 in this case (North)\n",
        "        return np.argmax(Q_table[state_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "097655b4-a28a-4719-a39e-4621c63555eb",
      "metadata": {
        "id": "097655b4-a28a-4719-a39e-4621c63555eb"
      },
      "outputs": [],
      "source": [
        "def update_Q_table(state, new_state, tcollected):\n",
        "        state_index = get_state_index(state, tcollected) #reminder: tcollected is a boolean that adds 1 or 0 to the state index\n",
        "        #example: state(4,0), state_index=40\n",
        "        new_state_index = get_state_index(new_state, tcollected)\n",
        "        #example (no treasure): new_state(4,1), new_state_index= 42\n",
        "        reward = step_cost\n",
        "        #every step starts with a negative reward\n",
        "\n",
        "        #               (0      ,       4)\n",
        "        if dungeon[new_state[0], new_state[1]] == GOAL:\n",
        "            Q_table[state_index, action] = goal_reward\n",
        "            #add to the table current location/state_index plus the decision taken to the qtable so future runs know where to go\n",
        "            #example: state(0,3), action east (0,1), add to qtable 100 points as reward\n",
        "            return False, tcollected #return false to end the game\n",
        "            #two return values, one for \"run\" to keep the game loop going, and one for treasure boolean\n",
        "\n",
        "\n",
        "        #checks if dragon is in a one block radius. If yes, game over\n",
        "        if any(dungeon[new_state[0] + dx, new_state[1] + dy] == DRAGON\n",
        "               for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "               if 0 <= new_state[0] + dx < GRID_SIZE\n",
        "               and 0 <= new_state[1] + dy < GRID_SIZE):\n",
        "            Q_table[state_index, action] = dragon_cost\n",
        "            return False, tcollected\n",
        "            #two return values, one for \"run\" to keep the game loop going, and one for treasure boolean\n",
        "\n",
        "\n",
        "        #if player current state is a treasure that hasn't been collected, add reward and mark as collected\n",
        "        if dungeon[new_state[0], new_state[1]] == TREASURE and not tcollected:\n",
        "            reward += treasure_reward\n",
        "            tcollected = True  # Mark treasure as collected\n",
        "            new_state_index = get_state_index(new_state, tcollected)\n",
        "        #if player current state is a fire, add fire cost\n",
        "        elif dungeon[new_state[0], new_state[1]] == FIRE:\n",
        "            reward += fire_cost\n",
        "        # --- Q-learning update ---\n",
        "\n",
        "        # 1. Store the current Q-value for this state-action pair\n",
        "        # Example: Q_table[40] = [0.5, -1.0, 0.0, 0.2] → old_value = Q_table[40, 0] = 0.5\n",
        "        old_value = Q_table[state_index, action]\n",
        "\n",
        "        # 2. Look ahead: get the best future value from the next state\n",
        "        # Example: Q_table[38] = [0.2, 0.0, 0.0, 0.3] → future_value = 0.3\n",
        "        future_value = np.max(Q_table[new_state_index])\n",
        "\n",
        "        # 3. Start the Q-value update:\n",
        "        # Decrease influence of the old value slightly, based on the learning rate (alpha)\n",
        "        # Example: (1 - 0.1) * 0.5 = 0.45\n",
        "        Q_table[state_index, action] = (1 - alpha) * old_value\n",
        "\n",
        "        # 4. Add in the new knowledge: the immediate reward + discounted future value\n",
        "        # Example: reward = 0.2, gamma = 0.9, so:\n",
        "        #          0.2 + 0.9 * 0.3 = 0.2 + 0.27 = 0.47\n",
        "        # Then:    Q += 0.1 * 0.47 = 0.047 → total becomes 0.497\n",
        "        Q_table[state_index, action] += alpha * (reward + gamma * future_value)\n",
        "\n",
        "        return True, tcollected\n",
        "        #returns true to keep the game going"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "383deaad-17cd-494b-a85e-bc0e967bf913",
      "metadata": {
        "id": "383deaad-17cd-494b-a85e-bc0e967bf913",
        "outputId": "8a8683ef-8a50-4874-c221-3f5f109a053f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:07<00:00, 7058.81it/s]\n"
          ]
        }
      ],
      "source": [
        "#game loop starting, episode number is 50000\n",
        "for episode in tqdm(range(episodes)): # tqdm adds a progress bar to track episode progress\n",
        "    state = (4, 0)  # Startin where the player is initally\n",
        "    tcollected = False # no treasure collected\n",
        "\n",
        "    run = True\n",
        "    while run:\n",
        "        #first get a list of the valid actions for the current state\n",
        "        valid_actions = get_valid_actions(state)\n",
        "        #choose an action based on exploration vs exploitation\n",
        "        action = choose_action(state, tcollected,\n",
        "                               valid_actions, Q_table, epsilon)\n",
        "        #apply the action to get the next stage (i.e go north or east from (4,0))\n",
        "        new_state = get_new_state(state, action)\n",
        "\n",
        "        # 4. Update the Q-table with the reward and future prediction\n",
        "        #    This also returns whether the game should continue, and if the treasure was collected\n",
        "        run, tcollected = update_Q_table(state,\n",
        "                                 new_state, tcollected)\n",
        "        # 5. Move the player to the new state for the next iteration\n",
        "        state = new_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f345191-0d2e-4ef7-a449-6de89a025c20",
      "metadata": {
        "id": "7f345191-0d2e-4ef7-a449-6de89a025c20"
      },
      "source": [
        "### Computed Policy\n",
        "After Q-Learning has been completed and the Q-table is filled with the optimal Q-values, we can extract the best action for each state and use it to create the policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4359dc61-5700-4088-b703-3f2e08a101c9",
      "metadata": {
        "id": "4359dc61-5700-4088-b703-3f2e08a101c9",
        "outputId": "24c5d9a7-924b-4c11-885f-07dafea60f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q-values (Treasure Not Collected):\n",
            "[  -inf  16.20  81.10 100.00 100.00]\n",
            "[  0.00   -inf  26.03  89.00 100.00]\n",
            "[  -inf  78.46  30.00  29.10  89.00]\n",
            "[ 61.65  69.61  78.46  70.19  79.10]\n",
            "[ 54.49  11.65  69.61  62.17  70.19]\n",
            "\n",
            "Optimal Policy (Treasure Not Collected):\n",
            "N E E E N\n",
            "N N S E N\n",
            "N E N E N\n",
            "E N N E N\n",
            "N N N N N\n",
            "\n",
            "Q-values (Treasure Collected):\n",
            "[  -inf  79.10  89.00 100.00 100.00]\n",
            "[  0.00   -inf  29.10  89.00 100.00]\n",
            "[  -inf  48.46  54.95  29.10  89.00]\n",
            "[ 48.46  54.95  62.17  70.19  79.10]\n",
            "[ 42.61  -1.54  54.95  62.17  70.19]\n",
            "\n",
            "Optimal Policy (Treasure Collected):\n",
            "N E E E N\n",
            "N N N N N\n",
            "N S S N N\n",
            "E E E E N\n",
            "N N N N N\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#policy: rule or strategy that tells the agent what action to take in each state.\n",
        "def extract_policy(Q_table, tcollected):\n",
        "    action_map = {\n",
        "        0: 'N',  # North\n",
        "        1: 'S',  # South\n",
        "        2: 'W',  # West\n",
        "        3: 'E'   # East\n",
        "    }\n",
        "    #changed werte to values for easier understanding in english\n",
        "\n",
        "    # Create a grid of 0s to later store the value of each cell (as floats), showing how valuable each cell's reward is\n",
        "    values = np.zeros((GRID_SIZE, GRID_SIZE), dtype=float)\n",
        "\n",
        "    # Create a grid to store the best action (as a string like 'N', 'E', etc.) for each cell\n",
        "    policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=str)\n",
        "\n",
        "\n",
        "    for x in range(GRID_SIZE):\n",
        "        for y in range(GRID_SIZE):\n",
        "            #extract the state index for each cell\n",
        "            state_index = get_state_index( (x, y), tcollected)\n",
        "\n",
        "            #determine what the best action for each individual cell is (highest Q-value)\n",
        "            best_action = np.argmax(Q_table[state_index])\n",
        "\n",
        "            #stores the best action as a letter from action_map ('N', 'S', 'W', 'E')\n",
        "            policy[x, y] = action_map[best_action]\n",
        "\n",
        "            #stores the highest Q-value (action's value) plus its step cost in the values map\n",
        "            #get_cost expects a tuple not an integer, change get_cose(state_index) into get_cost((x,y), tcollected)\n",
        "            #tcollected is used in the function but previous version of the code never sends it as a parameter\n",
        "            values[x, y] = np.max(Q_table[state_index])+get_cost((x,y), tcollected)\n",
        "\n",
        "    #returns the action taken, and its value\n",
        "    return values, policy\n",
        "\n",
        "\n",
        "    #this function prints the policies, once when no treasure is collected and once when it is\n",
        "def print_policy(values, policy, treasure_status):\n",
        "    #this line configures NumPy to print the numbers with 2 decimal places, aligned properly\n",
        "    #lambda functions can take multiple arguments but only one expression\n",
        "    #the :6 means that each number is 6 characters wide (i.e. ......) and doesnt exceed this space\n",
        "    np.set_printoptions(formatter={'all':lambda x: \"{:6.2f}\".format(x)})\n",
        "    #shows whether the treasure has been collected or not\n",
        "    print(f\"\\nQ-values (Treasure {'Collected' if treasure_status else 'Not Collected'}):\")\n",
        "    for row in values:\n",
        "        print(row.round(2)) #round to two decimal places\n",
        "\n",
        "    print(f\"\\nOptimal Policy (Treasure {'Collected' if treasure_status else 'Not Collected'}):\")\n",
        "    for row in policy:\n",
        "        print(\" \".join(row))\n",
        "\n",
        "values_not_collected, policy_not_collected = extract_policy(Q_table, tcollected=False)\n",
        "values_collected, policy_collected = extract_policy(Q_table, tcollected=True)\n",
        "\n",
        "print_policy(values_not_collected, policy_not_collected, treasure_status=False)\n",
        "print_policy(values_collected, policy_collected, treasure_status=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c12ddb2-0d3e-4fff-85a7-960260ffe93c",
      "metadata": {
        "id": "6c12ddb2-0d3e-4fff-85a7-960260ffe93c"
      },
      "source": [
        "### Executing the Policy\n",
        "The following code visualizes the policy and guides our player to the goal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "7ef102a9-26e8-43a0-b2ee-173413d61099",
      "metadata": {
        "id": "7ef102a9-26e8-43a0-b2ee-173413d61099"
      },
      "outputs": [],
      "source": [
        "def animate_step(dungeon, player_pos, tcollected):\n",
        "    display_dungeon = np.copy(dungeon)\n",
        "\n",
        "    px, py = player_pos\n",
        "\n",
        "    if tcollected and display_dungeon[px, py] == TREASURE:\n",
        "        display_dungeon[px, py] = EMPTY\n",
        "\n",
        "    display_dungeon[px, py] = PLAYER\n",
        "\n",
        "    top_border = \"┌───\" + \"┬───\" * (dungeon.shape[1] - 1) + \"┐\"\n",
        "    middle_border = \"├───\" + \"┼───\" * (dungeon.shape[1] - 1) + \"┤\"\n",
        "    bottom_border = \"└───\" + \"┴───\" * (dungeon.shape[1] - 1) + \"┘\"\n",
        "\n",
        "    print_grid = [top_border]\n",
        "\n",
        "    for x in range(GRID_SIZE):\n",
        "        row = '│'\n",
        "        for y in range(GRID_SIZE):\n",
        "            if tcollected and display_dungeon[x, y] == TREASURE:\n",
        "                row += symbol_map[EMPTY] + '│'  # Treasure is hidden once collected\n",
        "            else:\n",
        "                row += symbol_map[display_dungeon[x, y]] + '│'\n",
        "        print_grid.append(row)\n",
        "        if x < GRID_SIZE - 1:\n",
        "            print_grid.append(middle_border)\n",
        "\n",
        "    print_grid.append(bottom_border)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    for line in print_grid:\n",
        "        print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "a2affcc8-fb97-4f77-9efb-3b6965375887",
      "metadata": {
        "id": "a2affcc8-fb97-4f77-9efb-3b6965375887"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def animate_policy(Q_table, dungeon, start_pos=(4, 0), sleep_time=2):\n",
        "    werte_not_collected, policy_not_collected = extract_policy(Q_table, tcollected=False)\n",
        "    werte_collected, policy_collected = extract_policy(Q_table, tcollected=True)\n",
        "\n",
        "    player_pos = start_pos\n",
        "    tcollected = False\n",
        "    accumulated_reward = 0\n",
        "\n",
        "    px, py = player_pos\n",
        "    dungeon[px, py] = EMPTY  # Start by clearing the player’s initial position\n",
        "\n",
        "    while True:\n",
        "        animate_step(dungeon, player_pos, tcollected)\n",
        "\n",
        "        state_index = get_state_index(player_pos, tcollected)\n",
        "        best_action = np.argmax(Q_table[state_index])\n",
        "\n",
        "        new_pos = get_new_state(player_pos, best_action)\n",
        "        x, y = new_pos\n",
        "\n",
        "        if dungeon[x, y] == TREASURE:\n",
        "            tcollected = True\n",
        "            dungeon[x, y] = EMPTY  # Mark treasure as collected\n",
        "\n",
        "        if dungeon[x, y] == GOAL:\n",
        "            clear_output(wait=True)\n",
        "            animate_step(dungeon, new_pos, tcollected)\n",
        "            print(\"Reached goal!!\")\n",
        "            break\n",
        "\n",
        "        if dungeon[x, y] == DRAGON:\n",
        "            clear_output(wait=True)\n",
        "            animate_step(dungeon, new_pos, tcollected)\n",
        "            print(\"Died to the Dragon!!\")\n",
        "            break\n",
        "\n",
        "        player_pos = new_pos\n",
        "\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "copied_dungeon = np.copy(dungeon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e0506353-ed94-4d3e-bc02-456f0bd94687",
      "metadata": {
        "id": "e0506353-ed94-4d3e-bc02-456f0bd94687",
        "outputId": "5630a160-4f31-4d91-940a-0f0c910e7a8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌───┬───┬───┬───┬───┐\n",
            "│   │   │   │   │🧙 │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│🐉 │🔥 │🔥 │   │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │   │   │🔥 │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │   │   │   │   │\n",
            "├───┼───┼───┼───┼───┤\n",
            "│   │🔥 │   │   │   │\n",
            "└───┴───┴───┴───┴───┘\n",
            "Reached goal!!\n"
          ]
        }
      ],
      "source": [
        "animate_policy(Q_table, copied_dungeon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d5b8d1-7bd5-4b6f-96d7-7016fe7f43b8",
      "metadata": {
        "id": "c2d5b8d1-7bd5-4b6f-96d7-7016fe7f43b8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}